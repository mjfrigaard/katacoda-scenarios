---
title: "How to Create Column Charts in R with ggplot2"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    highlight: tango
    theme: cosmo
    df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse)
library(skimr)
library(knitr)
library(rmdformats)
library(fivethirtyeight)
knitr::opts_chunk$set(
  echo = TRUE,
  eval = TRUE,
  cache = FALSE,
  prompt = FALSE,
  tidy = FALSE,
  comment = "#>",
  message = FALSE,
  warning = FALSE,
  fig.path = "../figs/"
)
knitr::opts_knit$set(
  width = 78
)
base::options(
  tibble.print_max = 10,
  tibble.width = 78,
  scipen = 100000000
)
```

# intro.md

## Welcome!

Welcome to "How to Create Column Charts in R with ggplot2!" In this scenario, we will cover how to build a _column chart_ using two variables.

This scenario assumes you've done some data wrangling with [`tidyr` and `dplyr`](https://learning.oreilly.com/scenarios/-/9781492079064), and [data visualization with `ggplot2`](https://learning.oreilly.com/scenarios/-/9781492079071).

## Knowing your Data

It's best to start a project off with a "view of the forest from outside the trees." The technical term for this is [data lineage](https://en.wikipedia.org/wiki/Data_lineage#), which:

> "Includes the data origin, what happens to it, and where it moves over time."

Having a "bird's eye view" of the data ensures there weren't any problems with exporting or importing. Data lineage also means understanding where the data is coming from (e.g., a relational database, API, flat _.csv_ files, etc.).

## Tabular Data

Knowing some of the technical details behind a dataset lets us frame the questions or problems we're trying to tackle. In this scenario, we will use [tabular data](http://bit.ly/3aXXB4I) data (i.e., [spreadsheets](https://en.wikipedia.org/wiki/Spreadsheet)). Tabular data organizes information into columns and rows.

Let's load some data and get started!


# step1.md

## Initiate R

Launch an R console by clicking here: `R`


## Load packages

The package we'll use to view the entire dataset with R is [`skimr`](https://docs.ropensci.org/skimr/). We will install and load the following packages:

```{r , eval=FALSE}
install.packages(c("tidyverse", "skimr"))
library(tidyverse)
library(skimr)
```


## Navigating Katacoda

Let's take a quick tour of our Katacoda environment. In the following steps, we'll be running code, viewing output, and creating graphs. To accomplish this, we'll need to understand what tools are at our disposal.

### Sidebar

You're reading this in the *Sidebar*. All of the instructions are in the Sidebar, and at the bottom, you'll find a "Continue" button to take you to the next step. See the image below:

![](https://raw.githubusercontent.com/mjfrigaard/katacoda-scenarios/master/how-to-create-boxplots-in-r-with-ggplot2/img/sc-03-sidebar.png)


### Code Blocks

You will also find *code blocks* in the Sidebar. All the code blocks will run when you click on them (you've already run a few above!). See the following image for more examples:

![](https://raw.githubusercontent.com/mjfrigaard/katacoda-scenarios/master/how-to-create-boxplots-in-r-with-ggplot2/img/sc-03-code-blocks.png)

<h3>Terminal</h3>

The *Terminal* is where the R code from each code block will run, along with any text *output*. We can also use R interactively by typing them directly into the Terminal after the *prompt* (`>`).

Go ahead and try it by typing (or copying and pasting) the following commands:

```{r}
tidyverse::tidyverse_logo()
```

You should see the following output in the Terminal:

![](https://raw.githubusercontent.com/mjfrigaard/katacoda-scenarios/master/figs/sc-03-terminal-code.png)

### VS Code (Explorer)

When we create graphs, we will include the `ggplot2::ggsave()` function in the code block. This function allows us to save the graph image as a _.png_ file in the _VS Code explorer_.

### VS Code (ROOT)

Inside the VS Code explorer, you'll find a section labeled _ROOT_. _ROOT_ is a folder that contains our new graph files. We can open the _Graph file_ by clicking on them. See the following image for an example:

![](https://raw.githubusercontent.com/mjfrigaard/katacoda-scenarios/master/how-to-create-boxplots-in-r-with-ggplot2/img/sc-03-explorer-root.png)

The following image gives you an overview of this entire process:

![](https://raw.githubusercontent.com/mjfrigaard/katacoda-scenarios/master/how-to-create-boxplots-in-r-with-ggplot2/img/sc-03-code-run-png-view.png)

Now that we know how to navigate the Katacoda environment, we can start exploring data and building graphs!


# step2.md

## Before We Start: What Do We Expect to See?

Before starting a new project, we want to set some expectations. The questions we covered in the previous step help us understand what kind of data we'll be encountering. Sometimes we'll be dealing with unknown data, but we should know approximately how many columns and rows the new dataset will contain. We might know some basic information about the variable formats, too.

For example, we should see if we're getting date columns (`YYYY-MM-DD`), logical (`TRUE`, `FALSE`, `NA`), numerical measurements (integer (`1L`) or double (`1`)), or categorical data (character (`male` and `female`) or factor (`low`, `medium`, `high`)).

## Baseball Data

We're going to load a dataset to demonstrate a few ways to investigate a dataset's quality (or how well it matches our expectations).

This data comes from [Sean Lahman's Baseball Database](http://www.seanlahman.com/baseball-archive/statistics/).

Now, I am not going to assume everyone participating in this scenario is familiar with baseball. However, this exercise is arguably more rewarding if you are *not* a baseball fan. If you're working with data, part of your job to be interested in whatever you've been asked to analyze (even if it is only for the monetary reward):

> "...If you want to work in data visualization, you need to be relentlessly and systematically curious. You should try to get interested in anything and everything that comes your way." &ndash; Alberto Cairo, Knight Chair in Visual Journalism, University of Miami

Analyzing and visualizing data you're not familiar with is a chance to learn something new, and it puts you in a position to ask "out of the box" questions.

### Doing Your Homework

It's also essential to read any accompanying documentation for new datasets. If we read the documentation on the [Lahman website](http://www.seanlahman.com/files/database/readme2017.txt), we find out that `People` contains "*Player names, DOB, and biographical info.*"
The variables in `People` are presented in the following list:

<!-- **People table** -->

* `playerID` = A unique code assigned to each player; the `playerID` links the data in this file with records in the other files
* `birthYear` = Year player was born
* `birthMonth` = Month player was born
* `birthDay` = Day player was born
* `birthCountry` = Country where player was born
* `birthState` = State where player was born
* `birthCity` = City where player was born
* `deathYear` = Year player died
* `deathMonth` = Month player died
* `deathDay` = Day player died
* `deathCountry` = Country where player died
* `deathState` = State where player died
* `deathCity` = City where player died
* `nameFirst` = Player's first name
* `nameLast` = Player's last name
* `nameGiven` = Player's given name (typically first and middle)
* `weight` = Player's weight in pounds
* `height` = Player's height in inches
* `bats` = Player's batting hand (left, right, or both)
* `throws` = Player's throwing hand (left or right)
* `debut` = Date that player made first major league appearance
* `finalGame` = Date that player made first major league appearance (blank if still active)
* `retroID` = ID used by retrosheet
* `bbrefID` = ID used by Baseball Reference website

Most of the data pre-processing steps center around a single question: *Is this what I expected to see*? Reading the documentation gives you expectations about the data to confirm or refute (and then investigate).

Now that we have some background information on this new dataset, we will look at how well `People` meets our expectations.

Whenever we get a new data source, we should try to view the data in its native format (if possible). We can view the raw data on the [Github repository](https://resources.oreilly.com/katacoda/martin-frigaard/blob/master/data/People.csv).

## Load Data

Fortunately, we are also able to load the raw data directly into R using the `readr::read_csv()` function. We will load the `People` dataset into R using `readr::read_csv()`, and assign `"https://bit.ly/2ZMZjzv"` to the `file` argument:

```{r}
People <- readr::read_csv(file = "https://bit.ly/2ZMZjzv")
```


# step3.md

# Are We Seeing What We Expected?

Before creating any visualizations, we want a display that gives us an overview of the entire `People` dataset. In the previous step, we went over some of the `People` dataset documentation, so we know what to expect.

## Skimming Data

We'll be using the [`skimr` package](https://docs.ropensci.org/skimr/). `skimr` was is designed for:

> "Displaying summary statistics the user can skim quickly to understand their data."

In the following, we pass the `People` dataset to the `skimr::skim()` function to create `PeopleSkim`. We then use the `base::summary()` function to review the new object.

If this code looks unfamiliar to you, review the [introduction to `ggplot2` scenario](https://learning.oreilly.com/scenarios/-/9781492079071):

```{r}
PeopleSkim <- People %>%
  skimr::skim()
summary(PeopleSkim)
```


The output above shows a high-level summary of all the variables in the `People` dataset. We can see there are `20090` rows and `24` columns (`14` columns are `character`s, `2` columns are `Date`s, and `8` are `numeric`).

## Viewing Character Variables

The new `PeopleSkim` object gives us summary information to check against the documentation and help guide our data visualizations. We will start by viewing the variables according to their types in `People` using `skimr::yank()` (read the [function documentation on Github](https://github.com/ropensci/skimr/blob/master/R/reshape.R#L138)). The `skim_type` argument in `skimr::yank()` takes a variable type (`"character"`, `"numeric"`, or `"Date"`).

Run the following code to use `skimr::yank()` to view a `skim` of the `character` variables in the `People` dataset:

```{r}
PeopleSkim %>%
  skimr::yank(skim_type = "character")
```

We can see none of the data is missing (`n_missing` and `complete_rate`). `Skimr::skim()` also shows us the `min`, `max`, `empty`, `n_unique`, and `whitespace` for the `14` character values.

## Viewing Date Variables

Next, we use `skimr::yank()` to view a `skim` of the `Date` variables in the `People` dataset:

```{r}
PeopleSkim %>%
  skimr::yank(skim_type = "Date")
```

The `skim` of the `Date` variables shows us what data is missing (`n_missing` and `complete_rate`), along with the earliest (`min`), latest (`max`), and middle (`median`).

The number of unique (`n_unique`) dates prints to the next line. This behavior is because the terminal window has a width limit. If the Terminal output extends past this limit, the content gets printed to the line below:

![](https://raw.githubusercontent.com/mjfrigaard/katacoda-scenarios/master/how-to-create-bar-charts-in-r-with-ggplot2/img/n_unique-col.png)

## Do These Numbers Make Sense?

We can use these values for sanity checks. For example, the `n_unique` for `playerID` matches the total number of rows in `People`, which we should expect from the documentation (`playerID` = "*A unique code assigned to each player*"). The earliest dates for both `debut` and `finalGame` are in May of 1871 (which corresponds to the [first MLB game ever played](https://www.retrosheet.org/1stGame.htm)).

In the previous step, we viewed a `skim()` of the `"character"` and `"Date"` variables in the `People` dataset. We're going to continue 'skimming' the data and check it against our expectations.

## Viewing Numeric Variables

We'll use `skimr::yank()` and `skimr::focus()` to view the `n_missing` and `complete_rate` for the `"numeric"` variables in `People`:

```{r}
PeopleSkim %>%
  focus(n_missing, complete_rate) %>%
    yank("numeric")
```

The `complete_rate` for  `birthYear`, `birthMonth`, `birthDay`, `weight`, and `height` are over 90%. However, the `deathYear`, `deathMonth`, and `height` is under 50%. *Why do you think the data has missing values?*

The `skim()` output for the `"numeric"` variables give us a [set of summary statistics](https://en.wikipedia.org/wiki/Summary_statistics):

### Location Statistics

Location statistics include:

- The `mean` (or average) gives us the expected value for each variable.
- The median (as `p50`) or the 'center' value for each variable. Half of the values are above, and half are below.

```{r}
PeopleSkim %>%
  skimr::focus(numeric.mean, numeric.p50) %>%
    skimr::yank("numeric")
```


### Spread Statistics

Spread statistics include:

- The lowest value for each variable, or minimum (as `p0`).
- The highest value for each variable, or maximum (as `p100`).
- *Together, these two values can give us the range, which is the difference between the maximum and minimum values.*

```{r}
PeopleSkim %>%
  skimr::focus(numeric.p0, numeric.p100) %>%
    skimr::yank("numeric")
```

- The first quartile (as `p25`), which is the 'middle' of the data points *below* the median.
- The third quartile (as `p75`), which is the 'middle' of the data points *above* the median.
- *Together, these two values can give us the interquartile range (IQR), which is the difference between the third and first quartiles.*

```{r}
PeopleSkim %>%
  skimr::focus(numeric.p25, numeric.p75) %>%
    skimr::yank("numeric")
```


- The standard deviation (as `sd`), a measure of each variable's disbursement.
- *The standard deviation describes how far a variable's values are spread out around their mean.*

```{r}
PeopleSkim %>%
  skimr::focus(numeric.mean, numeric.sd) %>%
    skimr::yank("numeric")
```


These numbers can be challenging to make sense of by themselves. Fortunately, the `skimr::skim()` output comes with a `hist` column. The `hist` column is a small histogram for the `numeric` variables.

In the following, we use `skimr::focus()` and `skimr::yank()` to view the `mean`, standard deviation (`sd`), minimum (`p0`), median (`p50`), maximum (`p100`), and `hist` for the numeric variables in `People`:

```{r}
PeopleSkim %>%
  skimr::focus(numeric.mean, numeric.sd,
               numeric.p0, numeric.p50, numeric.p100,
               numeric.hist) %>%
    skimr::yank("numeric")
```

The `hist` column shows us a miniature distribution of the values in each numeric variable.

## Do These Numbers Make Sense?

As we can see, the majority of the missing values are in the variables with the `death` prefix (`deathDay`, `deathMonth`, and `deathYear`). The missing values in these variables make sense because, given the lowest `birthYear` value (`1820`), we should expect approximately half of the baseball players in the `People` dataset to be still alive.

We also notice an implausible value from the `skimr` output: the `weight` variable maximum value (`2125`). We can use `dplyr`'s `filter` and `select` functions to find the `nameGiven` for the abnormally high `weight` value:

```{r}
People %>%
  filter(weight == 2125) %>%
  select(nameGiven, birthMonth, birthDay, birthYear, weight)
```

Google the player's name. *What is his listed weight on Wikipedia?*

# step4.md

## Counting Top 10 Birth Countries

Column charts are great for representing counts, but first we need to know what numbers we're counting. In this example, we're interested in the 10 most common non-US birth countries for players in the `People` dataset.

The following code uses `dplyr::filter()` to find the non-US country of birth. We then use `dplyr::count()` to tally the countries, and `utils::head()` to only return the top 10.

We also use `dplyr::mutate()` with `forcats::fct_inorder()` to change the format of the `birthCountry` variable to a factor:

```{r}
# click to execute code
People %>%
  filter(birthCountry != "USA" & !is.na(birthCountry)) %>%
  count(birthCountry, sort = TRUE) %>%
  head(10) %>%
  mutate(birthCountry = fct_inorder(birthCountry))
```


If we're going to build a column graph from this data, we know we'll need a way to represent both the country's name and the number of times it occurs. In this case, we have two variables in this output: `birthCountry` and `n`.

Now we have an idea of the numbers we want to include in our visualization, so we're ready to create our graph. Building graphs with the `ggplot2` syntax can be expressed in the following template:

![](https://raw.githubusercontent.com/mjfrigaard/katacoda-scenarios/master/how-to-create-bar-charts-in-r-with-ggplot2/img/ggplot2-template.png)

If you need a refresher on `ggplot2` layers and mapping, check out the ["Build Beautiful, Customized Graphs and Charts in R with ggplot2" scenario](https://learning.oreilly.com/scenarios/-/9781492079071).


# step5.md

## Building a Column Chart

For our column chart, we will need to use the `geom_col()` function from `ggplot2` (we will include the code from the previous step, which creates a dataset with only the top 10 non-US birth countries).

To use `geom_col()`, we'll need to specify the variables we want mapped to the `x` and `y` axes (`geom_bar()` only takes a single categorical (or factor) variable). We also map `birthCountry` to `fill`, remove the legend with `show.legend = FALSE`, and add a label for the `y` axis with `ggplot2::labs()`.

```{r}
# click to execute code
gg_step5_col_01 <- People %>%
  filter(birthCountry != "USA" & !is.na(birthCountry)) %>%
  count(birthCountry, sort = TRUE) %>%
  head(10) %>%
  mutate(birthCountry = fct_inorder(birthCountry)) %>%
  ggplot(aes(x = birthCountry, y = n, fill = birthCountry)) +
  # remove legend
  geom_col(show.legend = FALSE) +
  # add label
  labs(title = "Top 10 Non-US birth countries for MLB players",
       subtitle = "Based on birthCountry",
       caption = "source: http://www.seanlahman.com/",
       x = NULL, y = "Number of birth countries")
# save
# ggsave(plot = gg_step5_col_01,
#         filename = "gg-step5-col-01.png",
#         device = "png",
#         width = 9,
#         height = 6,
#         units = "in")
gg_step5_col_01
```

To view the graph, open _gg-step5-col-01.png_ in the VS Code IDE above the Terminal console.

## Flip the Coordinates

If we find the `x` axis gets cluttered and difficult to read, we can pivot the columns' display with the `ggplot2::coord_flip()` function. Note that we also need to change the `forcats` function in `mutate()` to `fct_reorder()`:

```{r}
# click to execute code
gg_step5_col_02 <- People %>%
  filter(birthCountry != "USA" & !is.na(birthCountry)) %>%
  count(birthCountry, sort = TRUE) %>%
  head(10) %>%
  # reorder the birthCountry by n
  mutate(birthCountry = fct_reorder(birthCountry, n)) %>%
  ggplot(aes(x = birthCountry, y = n,
             fill = birthCountry)) +
  geom_col(show.legend = FALSE) +
  # flip coordinates
  coord_flip() +
  labs(title = "Top 10 Non-US birth countries for MLB players",
       subtitle = "Based on birthCountry",
       caption = "source: http://www.seanlahman.com/",
       x = NULL, y = "Number of birth countries")
# save
# ggsave(plot = gg_step5_col_02,
#         filename = "gg-step5-col-02.png",
#         device = "png",
#         width = 9,
#         height = 6,
#         units = "in")
gg_step5_col_02
```

To view the graph, open _gg-step5-col-02.png_ in the VS Code IDE above the Terminal console.

### Communication Tips

Bar charts and column charts display the counts for each different response in the categorical variable. We can help our audience interpret these graphs by always setting the count axis scale to zero, sorting the chart's values to make it easier to read, and flipping the `x` axis if it is difficult to read.


# finish.md

### How to Create Column Charts in R with ggplot2

In this scenario we covered how to:

1. `skim()` variables to get summary statistics
2. `filter` and `count()` variable values with `dplyr`
3. Build column charts with `ggplot2::geom_col()`
4. Flip axes coordinates with `ggplot2::coord_flip()`
4. Save graph images with `ggplot2::ggsave()`

### Thank you!

We've concluded the "How to Create Column Charts in R with ggplot2" scenario! Thank you for completing this scenario, and be sure to check out the other scenarios on the [O'Reilly Learning Platform](https://learning.oreilly.com/).

## Related Scenarios

* [Format and Shape Your Data in R with the Tidyverse](https://learning.oreilly.com/scenarios/-/9781492079064)
* [Built Beautiful, Customized Graphs and Charts in R with ggplot2](https://learning.oreilly.com/scenarios/-/9781492079071)
* [How to Create Bar Charts in R with ggplot2](https://learning.oreilly.com/scenarios/-/9781492079088)
* [How to Create Histograms in R with ggplot2](https://learning.oreilly.com/scenarios/-/9781098107406)
* [How to Create Density Plots in R with ggplot2](https://learning.oreilly.com/scenarios/-/9781098107543)
* [How to Create Ridgeline Plots in R with ggridges](https://learning.oreilly.com/scenarios/-/9781098107550)
* [How to Create Boxplots in R with ggplot2](https://learning.oreilly.com/scenarios/-/9781098107437)
* [How to Create Scatterplots in R with ggplot2](https://learning.oreilly.com/scenarios/-/9781098107444)
* [How to Create Line Plots in R with ggplot2](https://learning.oreilly.com/scenarios/-/9781098107413)
* [How to Create Jitter Plots in R with ggplot2](https://learning.oreilly.com/scenarios/-/9781098107734)
* [How to Create Small Multiples in R with ggplot2](https://learning.oreilly.com/scenarios/-/9781098107741)


